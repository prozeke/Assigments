{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;  1) Suppose you have N samples $x_1$, $x_2$.....$x_N$ from a univariate normal distribution\n",
    "              with unknown mean µ and known variance $σ^2$.<br> \n",
    "              &emsp;&emsp; Derive the MLE estimator for the mean µ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$MLE =\\underset{\\mu}{\\mathrm{argmax}}P(D|\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where $D$ denotes the dataset and $\\mu$ is the missing variable of the gaussian distribution. Lets first find out $P(D|\\mu)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is a normal distrubition with known variance $\\sigma^2$, its equation is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(D|\\mu) = \\Pi_{i=1}^n(2\\pi\\sigma^2)^{-1/2}e^{\\dfrac{-(x_i - \\mu)^2}{2\\sigma^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering all constants are positive, we can just get rid of them in $argmax$ function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{\\mu}{\\mathrm{argmax}}P(D|\\mu) = \\underset{\\mu}{\\mathrm{argmax}} \\Pi_{i=1}^ne^{-(x_i - \\mu)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since logarithm is a strictly increasing function we can write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{\\mu}{\\mathrm{argmax}} \\Pi_{i=1}^ne^{-(x_i - \\mu)^2} = \\underset{\\mu}{\\mathrm{argmax}}ln( \\Pi_{i=1}^ne^{-(x_i - \\mu)^2})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{\\mu}{\\mathrm{argmax}}ln( \\Pi_{i=1}^ne^{-(x_i - \\mu)^2}) = \\underset{\\mu}{\\mathrm{argmax}}\\Sigma_{i=1}^n-(x_i - \\mu)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the argmax of the last function, we need to take the derivative of it and equate to zero. Equation is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-\\Sigma_{i=1}^n2(x_i-\\mu)(-1) = 0$ which implies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Sigma_{i=1}^n(x_i) - n\\mu = 0 \\implies\\bar{x}n = n\\mu\\implies \\mu = \\bar{x}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, MLE estimator for $\\mu$ is the mean of the set N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;  2) Consider a dataset ($x_n , c_n $), n = 1, ..., N of binary attributes, $x^n_i ∈ 0, 1$, i = 1, ..., D and associated class label $c_n$ .<br> \n",
    " &emsp;&emsp; The number of datapoints from class $c = 0$ is denoted $n_0$ and the number from class $c = 1$ is denoted $n_1$ .<br> \n",
    "&emsp;&emsp;  Estimate $P(x_i = 1|c)\\equiv\\theta_i^c$ .<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finding maximum likelihood estimator we have to find $\\underset{\\theta_i^c}{\\mathrm{argmax}}P(x | C, \\theta_i^c)$ where $\\theta_i^c$ is the probability of $x_i = 1$ given class $c$. Consider that every point in x is considered as if it has only feature $x_i$ since we are trying to find $p(x_i = 1|c, \\theta_i^c)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets say that $\\alpha_{1,c}$ is the frequency of $x_i = 1$ given class $c$ and $\\alpha_{0,c}$ is the frequency of $x_i = 0$ given class c. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of  $P(x | C, \\theta_i^c)$ $=$ $\\binom{n_0}{\\alpha_{1,c}}(\\theta_i^c)^{\\alpha_{1,c}}(1-\\theta_1^c)^{\\alpha_{0,c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to find the value that maximizes the probobality of the dataset we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the combination is just a constant $\\underset{\\theta_i^c}{\\mathrm{argmax}}\\binom{n_0}{\\alpha_{1,c}}(\\theta_i^c)^{\\alpha_{1,c}}(1-\\theta_1^c)^{\\alpha_{0,c}} =  \\underset{\\theta_i^c}{\\mathrm{argmax}}(\\theta_i^c)^{\\alpha_{1,c}}(1-\\theta_1^c)^{\\alpha_{0,c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate $\\underset{\\theta_i^c}{\\mathrm{argmax}}$ for the log likelihood for the sake of simplifiying the derivative of the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{\\theta_i^c}{\\mathrm{argmax}}(\\theta_i^c)^{\\alpha_{1,c}}(1-\\theta_1^c)^{\\alpha_{0,c}} = \\underset{\\theta_i^c}{\\mathrm{argmax}}( \\alpha_{1,c}ln(\\theta_i^c) + \\alpha_{0,c}ln(1-\\theta_i^c))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fidnd the $\\underset{\\theta_i^c}{\\mathrm{argmax}}$ of the last equation, we need to take the derivative of that function and equate it to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of that function is $\\alpha_{1,c}\\dfrac{1}{\\theta_i^c} - \\alpha_{0,c}\\dfrac{1}{1 - \\theta_i^c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation becomes $\\alpha_{1,c}\\dfrac{1}{\\theta_i^c} - \\alpha_{0,c}\\dfrac{1}{1 - \\theta_i^c} = 0\\implies\\dfrac{\\theta_i^c}{1-\\theta_i^c} = \\dfrac{\\alpha_{1,c}}{\\alpha_{0,c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\theta_i^c}{1-\\theta_i^c} = \\dfrac{\\alpha_{1,c}}{\\alpha_{0,c}}\\implies \\theta_i^c = \\dfrac{\\alpha_{1,c}}{\\alpha_{1,c} + \\alpha_{0,c}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;3) Suppose that X is a discrete rrandom variable with the following probability mass<br>\n",
    "&emsp;&emsp;   function: where 0 ≤ θ ≤ 1 is a parameter. The following 10 independent ob-<br>\n",
    "&emsp;&emsp;   servations were taken from such a distribution: (3,0,2,1,3,2,1,0,2,1). What is the<br>\n",
    "&emsp;&emsp;   maximum likelihood estimate of θ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|X | 0 | 1 | 2 | 3 | \n",
    "| --- | --- | --- | --- | --- |\n",
    "|P(X) | $2\\theta/3$ | $\\theta/3$ | $2(1-\\theta/3$ |$(1-\\theta)/3$|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLE for this is $\\underset{\\theta}{\\mathrm{argmax}}P(X|\\theta) = \\underset{\\theta}{\\mathrm{argmax}}P(X=3)^2P(X=2)^3P(X=1)^3P(X=0)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of a function which is a bit hard to diffrentiate, we can find the maximum argument with respect to log likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underset{\\theta}{\\mathrm{argmax}}(2ln(P(X=3)) + 3ln(P(X=2)) + 3ln(P(X=1)) + 2ln(P(X=0))) = \\underset{\\theta}{\\mathrm{argmax}}(2ln((1-\\theta)/3) + 3ln(2(1-\\theta/3) + 3ln(\\theta/3) + 2ln(2\\theta/3) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the $\\underset{\\theta}{\\mathrm{argmax}}$ we need to equate the derivative of the function to zero and solve it. The equation is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$-2/\\theta - 3/\\theta + 2/\\theta + 3/\\theta = 0 \\implies \\theta/(\\theta - 1) = 1 \\implies\\theta = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A psychologist does a small survey on ’happiness’. Each respondent provides a vector\n",
    "with entries 1 or 0 corresponding to whether they answer ’yes’ to a question or ’no’,\n",
    "respectively. The question vector has attributes\n",
    "x = (rich, married, healthy)\n",
    "Thus, a response (1, 0, 1) would indicate that the respondent was ’rich’, ’unmarried’,\n",
    "’healthy’. In addition, each respondent gives a value c = 1 if they are content with their\n",
    "lifestyle, and c = 0 if they are not. The following responses were obtained from people\n",
    "who claimed also to be ’content’: (1, 1, 1), (0, 0, 1), (1, 1, 0), (1, 0, 1) and for ’not\n",
    "content’: (0, 0, 0), (1, 0, 0), (0, 0, 1), (0, 1, 0), (0, 0, 0) ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; 1)Using Naive Bayes, what is the probability that a person who is ’not rich’, ’married’\n",
    "and ’healthy’ is ’content’ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to naive bayes $P(content|rich',married,healty) = \\dfrac{P(rich'|content)P(married|content)P(healty|content)P(content)}{P(rich'|content)P(married|content)P(healty|content)P(content)+ P(rich'|content')P(married|content')P(healty|content')P(content')}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With respect to MLE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(content) = 4/9$, $P(content') = 5/9$<br> \n",
    "$P(rich|content) = 3/4$, $P(married|content) = 2/4$, $P(healty|content) = 3/4$<br>\n",
    "$P(rich|content') = 1/5$, $P(married|content') = 1/5$, $P(healty|content') = 1/5$.<br>\n",
    "Notice that $P(rich')=1-P(rich)$ and $P(rich'|content) = 1 - P(rich|content)$. The equation becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\frac{1}{4}\\frac{2}{4}\\frac{3}{4}\\frac{4}{9}}{\\frac{1}{4}\\frac{2}{4}\\frac{3}{4}\\frac{4}{9} + \\frac{4}{5}\\frac{1}{5}\\frac{1}{5}\\frac{5}{9}}\\approx 0.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp; What is the probability that a person who is ’not rich’ and ’married’ is ’content’ ?\n",
    "(That is, we do not know whether ot not they are ’healthy’.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(content|rich',married) = \\dfrac{P(rich'|content)P(married|content)P(content)}{P(rich'|content)P(married|content)P(content)+P(rich'|content')P(married|content')P(content')}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With respect to MLE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dfrac{\\frac{1}{4}\\frac{2}{4}\\frac{4}{9}}{\\frac{1}{4}\\frac{2}{4}\\frac{4}{9} + \\frac{4}{5}\\frac{1}{5}\\frac{5}{9}} \\approx 0.38$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Owerwiew of the Problem\n",
    "\n",
    "Nowadays, there are lots of fake news around us. These fake news not only wastes peoples time but also gives wrong information about what really is. The object of this homework is to make a detection system for classifying news as fake and real with respect o their headlines. We will use naive bayes alghorithm in order to achive this goal.<br>\n",
    "While finding this, we will use diffrent approaches and compare them. We will also have some number of tasks.\n",
    "\n",
    "#### Tasks\n",
    "\n",
    "##### Understanding The Data\n",
    "\n",
    "We will be predicting whether a headline is real or fake news from words that\n",
    "appear in the headline. We will give 3 examples of specific keywords that\n",
    "may be useful, together with statistics on how often they appear in real and fake\n",
    "headlines.\n",
    "\n",
    "\n",
    "##### Implementing Naive Bayes\n",
    "\n",
    "We will implement naive bayes for both unigram bag of words and bigram bag of words.\n",
    "\n",
    "##### Analyzing the Effect of Words\n",
    "\n",
    "a) Analyzing the effect of words<br>\n",
    "We will list:<br>\n",
    "&emsp; List the 10 words whose presence most strongly predicts that the news is real.<br>\n",
    "&emsp; List the 10 words whose absence most strongly predicts that the news is real.<br>\n",
    "&emsp; List the 10 words whose presence most strongly predicts that the news is fake.<br>\n",
    "&emsp; List the 10 words whose absence most strongly predicts that the news is fake.<br>\n",
    "\n",
    "Then reimplement second task with selecting strong words that implies a given class.\n",
    "\n",
    "b) Stopwords<br>\n",
    "A list of stopwords is available at sklearn.feature extraction.text.<br>\n",
    "We will list the 10 non-stopwords that most strongly predict that the news is<br>\n",
    "real, and the 10 non-stopwords that most strongly predict that the news is fake.\n",
    "\n",
    "c) Analyzing effect of the stopwords<br>\n",
    "We will answer these questions:<br>\n",
    "Why might it make sense to remove stop words when interpreting the model?<br>\n",
    "Why might it make sense to keep stop words?<br>\n",
    "\n",
    "##### Calculating the Accuracy\n",
    "\n",
    "We will calculate the accuracy of our solution with the formula:<br>\n",
    "$Accuracy = 100\\times\\dfrac{num_correct}{num_examples}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set\n",
    "\n",
    "1298 fake news headlines (which mostly include headlines of articles classified as bi-\n",
    "ased etc.) and 1968 real news headlines, where the fake news headlines are from\n",
    "https://www.kaggle.com/mrisdal/fake-news/data and real news headlines are from\n",
    "https://www.kaggle.com/therohk/million-headlines have been compiled. Data is cleaned\n",
    "by removing words from fake news titles that are not a part of the headline, removing\n",
    "special characters from the headlines, and restricting real news headlines to those after\n",
    "October 2016 containing the word trump.<br>\n",
    "We are provided with two txt files for train phase. One is for real news and the other is for fake news.The data set we use for training is consists of 1104 fake news and 1673 real news.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "We will use \"Naive Bayes Classifier\" algorithm with maximum likelihood estimation(MLE) approach. We will also use Bag of Words model for unigram and bigram in order to find nessecary probabilities.\n",
    "\n",
    "#### Naive Bayes Classifier\n",
    "\n",
    "Naive bayes classifier with MLE(Maximum Likelihood Estimation) is a probabilistic approach to classifying problem. Firstly, lets take a look to do bayes theorem.<br>\n",
    "$P(c|v) = \\dfrac{P(v|c)P(c)}{P(v)}$ In this equation, $P(c|v)$ is called posterior, $P(c)$ is called prior and $P(v|C)$ is called likelihood.<br>\n",
    "Considering the class as c and the vector of the predicted element as v, the goal of the naive bayes is to find $\\underset{C}{\\mathrm{argmax}P(c|v)}$ among the set of classes $C$ .<br>\n",
    "If one is able to find all of the elements on the left side of the equation, he/she can also manage to find $P(c|v)$. As a matter of fact, since oe needs to find argmax of c for the equation, he/she can neglect the term P(v) since it is just a positive constant independent from the variable c.<br>\n",
    "The maximum likelihood estimation approach gives us to assume some values for the terms $P(v|c)$ and $P(c)$. For binomial classifications, there are only two classes. Lets say $P(c_0) = \\theta$. This means that $P(c_1) = 1-\\theta$. Lets also assume that our data is represented as the random variable D.<br>\n",
    "$MLE =\\underset{\\mu}{\\mathrm{argmax}}P(D|\\theta)$ <br>\n",
    "With MLE approach we can find the most probable $P(c)$ and $P(v_i|c)$ where $v_i$ stands for a feature in vector v.<br>\n",
    "Naive bayes assumes that each element in the vector is independent from each other given the class(conditionally independent). This assumption allows us to formulate the naive bayes as<br>\n",
    "$\\underset{C}{\\mathrm{argmax}P(c|v)}\\implies\\underset{C}{\\mathrm{argmax}}(\\underset{V}{\\Pi}P(v|c)P(c))$\n",
    "Solution to this equation is the prediction of the naive bayes.<br>\n",
    "In our homework, the classes are binary and can be called as real and fake. Using MLE, we find the $P(real)$ as frequency of real news divided by the number of news. Similarly $P(fake)$ is frequency of fake news divided by the number of news. $P(word|class)$ is the frequency of the word divided by the number of words in that class.<br>\n",
    "In addition, we will use laplace smoothing while finding $P(class|word)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "\n",
    "#### Data Structers\n",
    "\n",
    "I have used the bag of words model to represent the data. I have defined  two class for that purpose. One named Bow and the other is named Word.<br>\n",
    "In Word class, I have five attributes. These are for the string value of the word, the probability of the word for each given class, and the log probabilities given each class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "\t## Word initialization. pr numbers will be firstly, the frequency of the word for each class. \n",
    "\t## They are initialized to one for laplace smoothing.\n",
    "\t## Then, they will be converted to probablitiies of the words given class.\n",
    "\t## Ex: logPrFake = Pr(word | fake).\n",
    "\t## At last, they will be kept as log probabilites.\n",
    "\t## Actual word (_word) is only kept for displaying it later on.\n",
    "\tdef __init__(self, _word):\n",
    "\t\tself.word = _word\n",
    "\t\tself.prReal = 0.0\n",
    "\t\tself.prFake = 0.0\n",
    "\t\tself.logPrFake = 1.0\n",
    "\t\tself.logPrReal = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Bow class, I have seven attributes which are for the frequencies of the real words and fake words, probabilities of being real and fake, log probabilities of being real and fake and lastly, a dictionary that consists of Word instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bow:\n",
    "\t## Bow stands for bag of words.\n",
    "\t## WordNum variables will be used when conditional probabilities are found.\n",
    "\t## Word initialization. pr* numbers will be firstly, the frequency of each class. \n",
    "\t## Then, pr* values will be converterd to be the probability of an entry is real or fake.\n",
    "\t## Ex. logPrReal = pr(Real).\n",
    "\t## At last, they will be kept as log probabilites.\n",
    "\t## It has the same variable name with Word attributes. DO NOT CONFUSE THEM!!!\n",
    "\tdef __init__(self):\n",
    "\t\tself.realWordNum = 0\n",
    "\t\tself.fakeWordNum = 0\n",
    "\t\tself.prReal = 0.0\n",
    "\t\tself.prFake = 0.0\n",
    "\t\tself.logPrReal = 0.0\n",
    "\t\tself.logPrFake = 0.0\n",
    "\t\tself.words = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also have several methods in this class that I had called for both prediction and additional tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\tdef get(self, word):\n",
    "\t\treturn words.get(word)\n",
    "\t\t\n",
    "\tdef addWord(self,_word,isReal):\n",
    "\t\tel = self.words.get(_word)\t## El is equal to word object retrieved from dictionary\n",
    "\t\tif(el == None):\t\t## If there is no object with that string, create one.\n",
    "\t\t\tel = Word(_word)\n",
    "\t\t\tself.words[_word] = el\t\t\t\n",
    "\t\t\tself.realWordNum += 1\t## this line is for laplace smoothing\n",
    "\t\t\tself.fakeWordNum += 1\t## It makes the assumption that each class has a non-visible entry which contains all the words. \n",
    "\t\tif(isReal):\n",
    "\t\t\tself.realWordNum += 1\n",
    "\t\t\tel.logPrReal += 1\t## logPrReal is now frequency. Notice that this pr stands for conditional probability\n",
    "\t\telse:\n",
    "\t\t\tself.fakeWordNum += 1\n",
    "\t\t\tel.logPrFake += 1\t## logPrFake is now frequency. Notice that this pr stands for conditional probability.\n",
    "\t\t\t\t\t\n",
    "\tdef addEntry(self, entry,isReal):\n",
    "\t\tif(len(entry) == 0):\n",
    "\t\t\treturn\n",
    "\t\tif(isReal):\n",
    "\t\t\tself.logPrReal += 1\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tself.logPrFake += 1\t\t\t\n",
    "\t\tfor _word in entry:\n",
    "\t\t\tself.addWord(_word, isReal)\n",
    "\n",
    "\t## Sets actual log probabilities and probabilities to neccesary fields.\n",
    "\t## It is a must to call this method after adding all the entries that our model has.\n",
    "\tdef findPr(self):\n",
    "\t\tentryNum = self.logPrReal + self.logPrFake\n",
    "\t\tself.prReal = self.logPrReal / entryNum\n",
    "\t\tself.prFake = self.logPrFake / entryNum\n",
    "\t\tself.logPrReal = math.log(self.logPrReal / entryNum)\n",
    "\t\tself.logPrFake = math.log(self.logPrFake / entryNum)\n",
    "\t\tfor key in self.words.keys():\n",
    "\t\t\tself.words[key].prReal = self.words[key].logPrReal / self.realWordNum\n",
    "\t\t\tself.words[key].prFake = self.words[key].logPrFake / self.fakeWordNum\n",
    "\t\t\tself.words[key].logPrReal = math.log(self.words[key].logPrReal / self.realWordNum)\n",
    "\t\t\tself.words[key].logPrFake = math.log(self.words[key].logPrFake / self.fakeWordNum)\n",
    "\t\t\t\n",
    "\t## Predicts entry. If predicts real news, return True. Otherwise, returns False\t\t\n",
    "\tdef predict(self, entry):\n",
    "\t\treal = self.logPrReal\n",
    "\t\tfake = self.logPrFake\n",
    "\t\tfor i in entry:\n",
    "\t\t\tw = self.words.get(i)\t\t\t\t\n",
    "\t\t\tif(w == None):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\treal += w.logPrReal\n",
    "\t\t\t\tfake += w.logPrFake\n",
    "\t\tif(real > fake):\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False\n",
    "\n",
    "\t## Finds the probability of the class given word.\n",
    "\t## Pr(C|word) so to say\n",
    "\t## word is an instance of Word. isReal is boolean.\n",
    "\tdef presence(self, word, isReal):\n",
    "\t\tif(isReal):\n",
    "\t\t\tprGivenClass = word.prReal\n",
    "\t\t\tprGivenOther = word.prFake\n",
    "\t\t\tprClass = self.prReal\n",
    "\t\t\tprOther = self.prFake\n",
    "\t\telse:\n",
    "\t\t\tprGivenClass = word.prFake\n",
    "\t\t\tprGivenOther = word.prReal\n",
    "\t\t\tprClass = self.prFake\n",
    "\t\t\tprOther = self.prReal\n",
    "\t\tprWord = prGivenClass * prClass + prGivenOther * prOther\n",
    "\t\treturn (prGivenClass * prClass) / prWord\n",
    "\n",
    "\t\n",
    "\t## Finds the probability of the class given the absence of the word.\n",
    "\t## Pr(C|word') so to say\n",
    "\t## word is an instance of Word. isReal is boolean.\n",
    "\tdef absence(self, word, isReal):\n",
    "\t\tif(isReal):\n",
    "\t\t\tprGivenClass = 1 - word.prReal\n",
    "\t\t\tprGivenOther = 1 - word.prFake\n",
    "\t\t\tprClass = self.prReal\n",
    "\t\t\tprOther = self.prFake\n",
    "\t\telse:\n",
    "\t\t\tprGivenClass = 1 - word.prFake\n",
    "\t\t\tprGivenOther = 1 - word.prReal\n",
    "\t\t\tprClass = self.prFake\n",
    "\t\t\tprOther = self.prReal\n",
    "\t\tprWord = prGivenClass * prClass + prGivenOther * prOther\n",
    "\t\treturn (prGivenClass * prClass) / prWord\n",
    "\t\t\n",
    "\t## Returns n words whose presence most strongly predicts that the news is real.\n",
    "\t## If isReal == False, then it returns the same words but for fake news.\n",
    "\tdef nPresence(self,n,isReal):\n",
    "\t\tret = []\n",
    "\t\twordList = self.words.values()\n",
    "\t\tnum = 0\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tif(num < n):\n",
    "\t\t\t\tel = El(word, self.presence(word,isReal))\n",
    "\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\telse:\n",
    "\t\t\t\tel = El(word, self.presence(word,isReal))\n",
    "\t\t\t\tif(el > ret[0]):\n",
    "\t\t\t\t\theapq.heappop(ret)\n",
    "\t\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\tnum += 1\n",
    "\t\treturn ret\n",
    "\n",
    "\t## Returns n words whose absence most strongly predicts that the news is real.\n",
    "\t## If isReal == False, then it returns the same words but for fake news.\n",
    "\tdef nAbsence(self,n,isReal):\n",
    "\t\tret = []\n",
    "\t\twordList = self.words.values()\n",
    "\t\tnum = 0\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tif(num < n):\n",
    "\t\t\t\tel = El(word, self.absence(word,isReal))\n",
    "\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\telse:\n",
    "\t\t\t\tel = El(word, self.absence(word,isReal))\n",
    "\t\t\t\tif(el > ret[0]):\n",
    "\t\t\t\t\theapq.heappop(ret)\n",
    "\t\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\tnum += 1\n",
    "\t\treturn ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also had two other classes that are named Data and El. Data is for holding the labeled data and El is for finding the top ten presence-absence problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\t## It is used to hold data with label\n",
    "\tdef __init__(self,_line, _label):\n",
    "\t\tself.line = _line\n",
    "\t\tself.label = _label\n",
    "\n",
    "class El:\n",
    "\t## El stands for element.\n",
    "\t## Will be used to find \"most smt. 10\" word problems.\n",
    "\t## It is created comparable to be compatable with python build-in heap structure \n",
    "\tdef __init__(self, _word, _pr):\n",
    "\t\tself.word = _word\n",
    "\t\tself.pr = _pr\n",
    "\t\n",
    "\tdef __lt__(self,other):\n",
    "\t\treturn self.pr < other.pr\n",
    "\t\t\n",
    "\tdef __gt__(self,other):\n",
    "\t\treturn self.pr > other.pr\n",
    "\t\t\n",
    "\tdef __eq__(self, other):\n",
    "\t\treturn self.pr > other.pr\n",
    "\t\t\n",
    "\tdef __le__(self,other):\n",
    "\t\treturn self.pr <= other.pr\n",
    "\t\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn self.word.word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data\n",
    "\n",
    "We are given 2 txt files.One file is for real news and other is for fake news. I have initialized the names of the files globally for the sake of simplicity.<br>\n",
    "Since we need to create 2 bag of words for both unigram and bigram, I made functions to convert a string to a list of bigram words and unigram words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Returns the entry for unigram implementation\n",
    "def unigramEntry(line):\n",
    "\tline = line.split(\" \")\n",
    "\treturn line\n",
    "\n",
    "## Returns entry for bigram implementation\n",
    "## If line contains of only one word, then it is same as unigramEntry\t\t\t\n",
    "def bigramEntry(line):\n",
    "\tline = line.split(\" \")\n",
    "\tret = []\t\t\t\t\n",
    "\tif(len(line) == 0):\n",
    "\t\treturn ret\n",
    "\tif(len(line) == 1):\n",
    "\t\treturn line\n",
    "\tfor i in range(len(line) - 1):\n",
    "\t\tret.append(line[i] + \" \" + line[i + 1])\n",
    "\treturn ret\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading is done with readAll function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAll(stopWords = dict()):\n",
    "\tfilePtr = open(REAL_FILE, \"r\" )    ## Real_FILE is a global variable\t\t\t\n",
    "\tisReal = True\t\t\n",
    "\tbows = [Bow(), Bow()]\t## bows[0] is for Unigram bow and bows[1] is for bigram bow.\t\t\n",
    "\tread(filePtr,stopWords,bows,isReal)\t\t\n",
    "\tfilePtr.close()\n",
    "\tfilePtr = open(FAKE_FILE, \"r\" )\t## FAKE_FILE is a global variable\t\t\n",
    "\tisReal = False\t\t\n",
    "\tread(filePtr,stopWords,bows,isReal)\t\n",
    "\tfilePtr.close()\t\t\n",
    "\tbows[0].findPr()  ## This is a must to call function after initializing \n",
    "\tbows[1].findPr()  ## It sets the probability related attributes to correct values\n",
    "\treturn bows\n",
    "\n",
    "def read(filePtr, stopWords, bows, isReal):\n",
    "\tline = filePtr.readline()\n",
    "\twhile(line):\n",
    "\t\tline = clearLine(line,stopWords)\n",
    "\t\tif(len(line) == 0):\n",
    "\t\t\tline = filePtr.readline()\n",
    "\t\t\tcontinue\t\n",
    "\t\tuEntry = unigramEntry(line)\n",
    "\t\tbiEntry = bigramEntry(line)\n",
    "\t\tbows[0].addEntry(uEntry,isReal)\n",
    "\t\tbows[1].addEntry(biEntry,isReal)\n",
    "\t\tline = filePtr.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks\n",
    "\n",
    "##### 1) Understanding the Data\n",
    "\n",
    "I found three example of words which may be helpful for classification are climate, refugee, military<br>\n",
    "Frequency of climate in real = 24&emsp;Frequency of climate in fake = 0<br>\n",
    "Frequency of refugee in real = 19&emsp;Frequency of refugee in fake = 0<br>\n",
    "Frequency of military in real = 18&emsp;Frequency of military in fake = 3<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2)Implementing Naive Bayes\n",
    "\n",
    "In order to implement naive bayes algorithm, I had used a bag of words model. Words in the bag has two conditional probabilty, P(word|real) and P(word|fake). I also hold the log likelihood of that probability. In addition, I have used laplace smoothing while finding conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "\t## Word initialization. logPr numbers will be firstly, the frequency of the word for each class. \n",
    "\t## They are initialized to one for laplace smoothing.\n",
    "\t## Then, they will be converted to probablitiies of the words given class.\n",
    "\t## Ex: logPrFake = Pr(word | fake).\n",
    "\t## At last, they will be kept as log probabilites.\n",
    "\t## Actual word (_word) is only kept for displaying it later on.\n",
    "\tdef __init__(self, _word):\n",
    "\t\tself.word = _word\n",
    "\t\tself.prReal = 0.0\n",
    "\t\tself.prFake = 0.0\n",
    "\t\tself.logPrFake = 1.0\n",
    "\t\tself.logPrReal = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, while computing the posterior, I used log likelihood to find it. The reason behind this is that posterior probability becomes very small as the headline has more words in it. This is a problem since very small numbers on can't be represented accurately by computers. The function below is a method of the Bow class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t## Predicts entry. If predicts real news, return True. Otherwise, returns False\t\t\n",
    "\tdef predict(self, entry):\n",
    "\t\treal = self.logPrReal\n",
    "\t\tfake = self.logPrFake\n",
    "\t\tfor i in entry:\n",
    "\t\t\tw = self.words.get(i)\t\t\t\t\n",
    "\t\t\tif(w == None):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\telse:\n",
    "\t\t\t\treal += w.logPrReal\n",
    "\t\t\t\tfake += w.logPrFake\n",
    "\t\tif(real > fake):\n",
    "\t\t\treturn True\n",
    "\t\telse:\n",
    "\t\t\treturn False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Analyzing the Effect of Words\n",
    "\n",
    "In this question, we need to find the probability of the class given word is precense or absence.<br>\n",
    "The formula is $P(c|word)$. With respect to bayesian theorem, formula can be written as follows:<br>\n",
    "For presence: $P(c|word) \\equiv \\dfrac{P(word|c)P(c)}{P(word)}$<br>\n",
    "For absence: $P(c|word) \\equiv \\dfrac{(1-P(word|c)P(c))}{1-P(word)}$<br>\n",
    "I know the values of $P(word|c)$ and $P(c)$ because I kept these values inside Bow class and Word Class. Therefore, all I need is to find $P(word)$.<br>\n",
    "With respect to bayesian theorem, $P(word) \\equiv P(word|c)P(c) + P(word|c')P(word)$.<br>\n",
    "The method below is for finding $P(class|word)$ and is a method of Bow class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t## Finds the probability of the class given word.\n",
    "\t## Pr(C|word) so to say\n",
    "\t## word is an instance of Word. isReal is boolean.\n",
    "\tdef presence(self, word, isReal):\n",
    "\t\tif(isReal):\n",
    "\t\t\tprGivenClass = word.prReal\n",
    "\t\t\tprGivenOther = word.prFake\n",
    "\t\t\tprClass = self.prReal\n",
    "\t\t\tprOther = self.prFake\n",
    "\t\telse:\n",
    "\t\t\tprGivenClass = word.prFake\n",
    "\t\t\tprGivenOther = word.prReal\n",
    "\t\t\tprClass = self.prFake\n",
    "\t\t\tprOther = self.prReal\n",
    "\t\tprWord = prGivenClass * prClass + prGivenOther * prOther\n",
    "\t\treturn (prGivenClass * prClass) / prWord\n",
    "\n",
    "\t\n",
    "\t## Finds the probability of the class given the absence of the word.\n",
    "\t## Pr(C|word') so to say\n",
    "\t## word is an instance of Word. isReal is boolean.\n",
    "\tdef absence(self, word, isReal):\n",
    "\t\tif(isReal):\n",
    "\t\t\tprGivenClass = 1 - word.prReal\n",
    "\t\t\tprGivenOther = 1 - word.prFake\n",
    "\t\t\tprClass = self.prReal\n",
    "\t\t\tprOther = self.prFake\n",
    "\t\telse:\n",
    "\t\t\tprGivenClass = 1 - word.prFake\n",
    "\t\t\tprGivenOther = 1 - word.prReal\n",
    "\t\t\tprClass = self.prFake\n",
    "\t\t\tprOther = self.prReal\n",
    "\t\tprWord = prGivenClass * prClass + prGivenOther * prOther\n",
    "\t\treturn (prGivenClass * prClass) / prWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also need to find 10 words for precense and absence and for each class. I used a heap based function for this. The elements of the heap is a class which I named El."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class El:\n",
    "\t## El stands for element.\n",
    "    ## word is an instance of class Word and pr is the probability of that word(P(word)).\n",
    "\t## Will be used to find \"most smt. 10\" word problems.\n",
    "\t## It is created comparable to be compatable with python build-in heap structure \n",
    "\tdef __init__(self, _word, _pr):\n",
    "\t\tself.word = _word\n",
    "\t\tself.pr = _pr\n",
    "\t\n",
    "\tdef __lt__(self,other):\n",
    "\t\treturn self.pr < other.pr\n",
    "\t\t\n",
    "\tdef __gt__(self,other):\n",
    "\t\treturn self.pr > other.pr\n",
    "\t\t\n",
    "\tdef __eq__(self, other):\n",
    "\t\treturn self.pr > other.pr\n",
    "\t\t\n",
    "\tdef __le__(self,other):\n",
    "\t\treturn self.pr <= other.pr\n",
    "\t\t\n",
    "\tdef __str__(self):\n",
    "\t\treturn self.word.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the heap based solution for absence and presence. This functions are methods of Bow class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t## Returns n words whose presence most strongly predicts that the news is real.\n",
    "\t## If isReal == False, then it returns the same words but for fake news.\t\n",
    "    def nPresence(self,n,isReal):\n",
    "\t\tret = []\n",
    "\t\twordList = self.words.values()\n",
    "\t\tnum = 0\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tif(num < n):\n",
    "\t\t\t\tel = El(word, self.presence(word,isReal))\n",
    "\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\telse:\n",
    "\t\t\t\tel = El(word, self.presence(word,isReal))\n",
    "\t\t\t\tif(el > ret[0]):\n",
    "\t\t\t\t\theapq.heappop(ret)\n",
    "\t\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\tnum += 1\n",
    "\t\treturn ret\n",
    "\n",
    "\t## Returns n words whose absence most strongly predicts that the news is real.\n",
    "\t## If isReal == False, then it returns the same words but for fake news.\n",
    "\tdef nAbsence(self,n,isReal):\n",
    "\t\tret = []\n",
    "\t\twordList = self.words.values()\n",
    "\t\tnum = 0\n",
    "\t\tfor word in wordList:\n",
    "\t\t\tif(num < n):\n",
    "\t\t\t\tel = El(word, self.absence(word,isReal))\n",
    "\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\telse:\n",
    "\t\t\t\tel = El(word, self.absence(word,isReal))\n",
    "\t\t\t\tif(el > ret[0]):\n",
    "\t\t\t\t\theapq.heappop(ret)\n",
    "\t\t\t\t\theapq.heappush(ret,el)\n",
    "\t\t\tnum += 1\n",
    "\t\treturn ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real top ten presence unigram:<br>\n",
    "['refugee', 'paris', 'australia', 'ban', 'north', 'korea', 'turnbull', 'travel', 'trumps', 'climate']<br>\n",
    "\n",
    "fake top ten presence unigram:<br>\n",
    "['steal', 'dr', 'woman', 'duke', '7', 'soros', 'u', '3', 'breaking', 'black']<br>\n",
    "\n",
    "real top ten absence unigram:<br>\n",
    "['if', 'it', 'just', 'and', 'clinton', 'a', 'the', 'hillary', 'is', 'you']<br>\n",
    "\n",
    "fake top ten absence unigram:<br>\n",
    "['travel', 'korea', 'turnbull', 'ban', 'says', 'us', 'north', 'donald', 'trumps', 'trump']<br>\n",
    "\n",
    "\n",
    "real top ten presence bigram:<br>\n",
    "['wall st', 'trump defends', 'trumps travel', 'us election', 'james comey', 'donald trumps', 'trump travel', 'north korea', 'travel ban', 'malcolm turnbull']<br>\n",
    "\n",
    "fake top ten presence bigram:<br>\n",
    "['breaking trump', 'fame star', 'george soros', 'will win', 'trump supporter', 'voting for', 'trump won', 'if trump', 'i m', 'daily wire']<br>\n",
    "\n",
    "real top ten absence bigram:<br>\n",
    "['a trump', 'i m', 'in the', 'anti trump', 'trump supporter', 'trump is', 'for trump', 'hillary clinton', 'if trump', 'comment on']<br>\n",
    "\n",
    "fake top ten absence bigram:<br>\n",
    "['malcolm turnbull', 'wall street', 'us election', 'travel ban', 'trump travel', 'donald trumps', 'north korea', 'donald trump', 'trump says', 'white house']<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4) Stop Words\n",
    "\n",
    "Here is the list of 10 non-stopwords that most strongly predict that the news is\n",
    "real, and the 10 non-stopwords that most strongly predict that the news is\n",
    "fake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "real top ten presence unigram:<br>\n",
    "['refugee', 'paris', 'australia', 'climate', 'north', 'trumps', 'travel', 'korea', 'ban', 'turnbull']<br>\n",
    "\n",
    "fake top ten presence unigram:<br>\n",
    "['dr', 'steal', 'woman', 'duke', '7', 'soros', 'breaking', 'u', '3', 'black']<br>\n",
    "\n",
    "real top ten presence bigram:<br>\n",
    "['executive order', 'trump tax', 'james comey', 'trump defends', 'trumps travel', 'trump travel', 'donald trumps', 'travel ban', 'north korea', 'malcolm turnbull']<br>\n",
    "\n",
    "fake top ten presence bigram:<br>\n",
    "['pro trump', 'liberty writers', 'fame star', 'news network', 'endingfed news', 'breaking trump', 'daily wire', 'trump supporter', 'trump won', 'george soros']<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5) Analyzing the Effect of Stop Words\n",
    "\n",
    "For train test, using given stopwords didn't change result at all. However, it slightly decreased accuracy. However, in actual test, it significantly decreased accuracy for both unigram and bigram. Here are the accuracy results before and after the usage of stop words in actual test.<br><br>\n",
    "For unigram Bow:<br>\n",
    "Without stop words: 86.30&emsp;With stop words: 84.45<br>\n",
    "<br>\n",
    "For bigram Bow:<br>\n",
    "Without stop words: 84.46&emsp;With stop words: 78.12<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6) Calculation of Accuracy\n",
    "\n",
    "Accuracy for unigram Bow withouth stop words on train data: 94.67<br>\n",
    "Accuracy for bigram Bow with stop words on train data: 99.67<br>\n",
    "Accuracy for unigram Bow with stop words on train data: 94.67<br>\n",
    "Accuracy for bigram Bow with stop words on train data: 99.06<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Bag of words model is a good method for this kind of classification. I had good accuracy for both training error and test error. However, it may have worse results if news are, for example, ten years before the trained data.<br>\n",
    "Stop words we used for this homework can be extended. For example, numerical strings can also be disgarded from data such as 3 or 7.<br>\n",
    "Also stemming can be useful for bag of words model. For example; immigration, immigrant, immigrants, immigrate is referring to similar concepts but they are regarded as completely diffrent words in my implementation. Maybe, it would make more sence to regard them as same word instead of diffrent words.<br>\n",
    "In conclusion, I managed to classify news as real and fake with my implementation but my impleentation might get better with the use of techniques I mentioned above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
