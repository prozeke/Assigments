{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "## Part 1\n",
    "### Theory Questions\n",
    "    1) What are the diffrences between logistic regression and linear regression?\n",
    "            \n",
    "            Linear regression is for continous problems whereas logistic regression is a classifier.\n",
    "        Logistic regression is a probabilistic approach. It calculates the probabilities of each class and tries to minimize\n",
    "        its error. Linear regression is an analaytic approach. It tries to find a line that is fits best to the data points.\n",
    "        This approaches results different objective functions. Logistic regression uses cross-entropy error function and\n",
    "        linear regression uses mean squared error function.\n",
    "        \n",
    "    2)What are differences between logistic regression and naive bayes methods?\n",
    "           \n",
    "           Logistic regression tries to find posterior directly. This makes logistic regression a discriminitive model.\n",
    "        Naive bayes first finds prior probability for each feature. Then, with the assumption of conditional independency\n",
    "        of each variable, it finds posterior. This makes naive bayes a generative model.\n",
    "            Naive bayes reaches to its asymptotic error faster. However, logistic regression ha a lower asymptotic error.\n",
    "        This makes naive bayes a better algorithm for small datasets and makes logistic regression better for large datasets.\n",
    "            As mentioned above, naive bayes makes the assumption of marginally conditionally independence for every feature\n",
    "        Another diffrence is that logistic regression does not have such an assumption.\n",
    "\n",
    "    3)Which of the following statements are true?\n",
    "        \n",
    "        True ones are:\n",
    "                A two layer (one input layer, one output layer; no hidden layer) neural network\n",
    "            can represent the XOR function.\n",
    "            \n",
    "                Any logical function over binary-valued (0 or 1) inputs x 1 andx 2 can be\n",
    "            (approximately) represented using some neural network.\n",
    "                \n",
    "                The activation values of the hidden units in a neural network, with the sigmoid\n",
    "            activation function applied at every layer, are always in the range (0, 1).\n",
    "            It is hard to say anything on the last one. Activation values of the hidden units does not needs to be\n",
    "            neccesarily between (0,1). But when application function is applied to activasions, outputs would range\n",
    "            between(0,1). I added this one as true too because I didn't understand which one was asking.\n",
    "            \n",
    "    4)How to decide the number of hidden layers and nodes in a hidden layer?\n",
    "        \n",
    "        They say that, in most cases, 1 hidden layer is sufdicent enough. The number of neurons in one layer should be\n",
    "    between the output class size and feauture size.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Classification of Flowers using Neural Networks\n",
    "### Owerview of the Problem\n",
    "        Neural networks can express many data distributions. In this homework, we are expected to construct neural network to classify five class\n",
    "    of flowers. First, we will implement a single layer neural network. Then, we extend the neural network to a multilayer network.\n",
    "### Data Set\n",
    "        We will use this https://www.kaggle.com/alxmamaev/flowers-recognition/downloads/flowers.zip/1 dataset. In addition to that\n",
    "    Necva Bölücü hoca shared a code that converts pictures to vectors with 768 features. I have used that vectors as inputs.\n",
    "\n",
    "### Algorithm\n",
    "        Neural networks consists of layers. Each layer has an activation function and some number of neurons. Each neuron has a weight vector.\n",
    "    In addition to that, a neural network has an objective function. The goal of training is to minimize objective function.\n",
    "    It is really import to know the overall structure of a neural network in order to understand the algorithm. \n",
    "    \n",
    "        The training algorithm has two main parts. These parts are feed forward and back propagation.We use these parts consecutively in order\n",
    "    to update the weights of each neuron. We first feed forward, and then back propagete to find derivatives.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"network.png\" width=\"1000\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed Forward\n",
    "&emsp;For each layer, one forward is finding the input of the next layer, with respect to its weights and activation function.<br>\n",
    "Lets suppose that we are at layer j and find the inputs of the next layer. $w_{ji}$ denotes the i'th neurons weight vector and $z_j$ is the inputs<br>\n",
    "of the the j'th layer and $z_{ki}$ is the i'th input of the next layer. $h_j()$ is j'th layers activation. Then one forward means finding the $z_k$ vector.<br>\n",
    "$z_kj = h_j(z_j^T.w_{ji})$\n",
    "&emps;Finding the inputs f each layer is feed forward. At last layer, we find the inputs of the error function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back Propagation\n",
    "&emsp;In order to understand back propagation lets consider a middle layer.<br>\n",
    "Lets suppose that $z_j$ is the inputs of the j'th layer and $a_k$ is the inputs of the activations of the layer after. I adopt the<br>\n",
    "same notation above for the other parts. Lets say $E$ denotes the error function.<br>\n",
    "$\\dfrac{\\partial E}{\\partial w_{ji}} = \\dfrac{\\partial E}{\\partial a_j}\\dfrac{\\partial a_j}{\\partial w_{ji}}$<br>\n",
    "lets say  $\\dfrac{\\partial E}{\\partial a_j} = \\delta_j$ and $\\dfrac{\\partial E}{\\partial a_k} = \\delta_k$ then<br>\n",
    "$\\dfrac{\\partial a_j}{\\partial w_{ji}} = z_j\\implies\\dfrac{\\partial E}{\\partial w_{ji}} = \\delta_jz_{ji}$<br>\n",
    "$\\delta_j = \\underset{K}{\\Sigma}\\dfrac{\\partial E}{\\partial a_k}\\dfrac{\\partial a_k}{\\partial a_j}$<br>\n",
    "$\\delta_j = h'(a_j)\\underset{K}{\\Sigma}\\delta_kw_{kj}$<br>\n",
    "We can find the $\\delta_{last}$ with help of feed forward. Back propagation is finding each weights derivative with respect to error<br>\n",
    "function with recursively fiding the $\\delta$ before starting from $\\delta_{last}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implentation\n",
    "#### Data Structures\n",
    "&emsp;I have used data structures to define activasion functions. Each activason function has its class with its activation function<br>\n",
    "and derivative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "\tdef __init__(self):\n",
    "\t\treturn\n",
    "\t\n",
    "\tdef activate(self, x):\n",
    "\t\treturn 1.0/(1 + np.exp(-x)) \t\t\n",
    "\t\n",
    "\tdef derivative(self,x):\n",
    "\t\treturn  x * (1 - x)\n",
    "\n",
    "class Tanh:\n",
    "\tdef __init__(self):\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef activate(self, x):\n",
    "\t\tval1 = np.exp(x)\n",
    "\t\tval2 = np.exp(-x)\n",
    "\t\treturn (val1 - val2) / (val1 + val2)\n",
    "\n",
    "\tdef derivative(self, x):\n",
    "\t\treturn 1-x**2\n",
    "\t\t\n",
    "class ReLu:\n",
    "\tdef __init__(self):\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef activate(self, x):\n",
    "\t\tif(x > 0):\n",
    "\t\t\treturn x\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\t\t\t\n",
    "\tdef derivative(self, x):\n",
    "\t\tif(x > 0):\n",
    "\t\t\treturn 1\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\t\t\t\n",
    "class NoActivation:\n",
    "\tdef __init__(self):\n",
    "\t\treturn \n",
    "\t\n",
    "\tdef activate(self, x):\n",
    "\t\treturn x\n",
    "\t\t\t\n",
    "\tdef derivative(self, x):\n",
    "\t\treturn 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also have a layer class. And a network class which consists of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\t\t# weightNum is the number of weights each layer has\n",
    "\t\t# neuronNum is neuron number\n",
    "\t\t# func is layers activation function class\n",
    "\t\t# bias is the bias vector of this layer   \n",
    "\t\t# inpV is the input vector for that layer\n",
    "\t\tdef __init__(self, weightNum, neuronNum, func):\n",
    "\t\t\tself.bias = np.random.randn(neuronNum) * 0.01\n",
    "\t\t\tself.func = func\n",
    "\t\t\tself.inpV = None\n",
    "\t\t\tself.weights = np.random.randn(neuronNum, weightNum) * 0.01\n",
    "\t\t\treturn\t\n",
    "\t\t\n",
    "\t\t# returns the input of the next layer\n",
    "\t\tdef forward(self):\n",
    "\t\t\tret = np.zeros(len(self.weights))\n",
    "\t\t\tfor neuron in range(len(self.weights)):\n",
    "\t\t\t\tz = np.dot(self.weights[neuron], self.inpV) + self.bias[neuron]\n",
    "\t\t\t\tret[neuron] = self.func.activate(z)\n",
    "\t\t\treturn ret\n",
    "\t\t\n",
    "\t\t# inpV are the input vector of the layer after this layer.\n",
    "\t\t# Finds the derivative of the layer.\n",
    "\t\t# Makes activations the derivative vector where activasions[i] is the derivative of the\n",
    "\t\t# i'th neurons activasion function with respect to input of that activation function. \t\n",
    "\t\tdef layerDerivative(self, inpV):\n",
    "\t\t\tret = np.array(inpV)\n",
    "\t\t\tfor i in range(len(inpV)):\n",
    "\t\t\t\tret[i] = self.func.derivative(inpV[i])\n",
    "\t\t\treturn ret\n",
    "\t\t\n",
    "\t\t# inpV are the input vector of the layer after this layer.\t\n",
    "\t\t# weightBefore is weights before this level\n",
    "\t\t# lambdaBefore is the lambda for\n",
    "\t\t# returns delta vector for that class \n",
    "\t\tdef findDelta(self, inpV, weightBefore, deltaBefore):\n",
    "\t\t\tder = self.layerDerivative(inpV)\n",
    "\t\t\tdelta = np.dot(weightBefore.transpose(),deltaBefore)\n",
    "\t\t\tdelta = np.multiply(delta, der)\n",
    "\t\t\treturn delta\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "class NeuralNetwork:\n",
    "\t# NeuronNumV[i] is the number of neurons in i'th layer\n",
    "\t# FunctionV[i] is the activasion function in i'th layer\n",
    "\t# FunctionV elements can be \"tanh\", \"sigmoid\", \"relu\"\n",
    "\t# Default is a neuron with no activasion function\n",
    "\t# ClassNum is the number of output classes\n",
    "\t# If neuronNumV and functionV is an empty listi then it will create a single layer network.\n",
    "\tdef __init__(self, neuronNumV, functionV, classNum, featureNum):\n",
    "\t\trelu = ReLu()\n",
    "\t\tsigmoid = Sigmoid()\n",
    "\t\ttanh = Tanh()\n",
    "\t\tdefault = NoActivation()\n",
    "\t\tself.layers = []\n",
    "\t\tfdict = {\"relu\": relu, \"sigmoid\": sigmoid, \"tanh\": tanh}\n",
    "\t\tif(len(neuronNumV) != len( functionV)):\n",
    "\t\t\tprint(\"Each layer function must be specified\")\n",
    "\t\t\tprint(\"Error in NeuralNetwork class while initializing object\")\n",
    "\t\t\tquit()\n",
    "\t\tfor i in range(len(functionV)):\n",
    "\t\t\tfunc = fdict.get(functionV[i])\n",
    "\t\t\tif(func):\n",
    "\t\t\t\tfunctionV[i] = func\n",
    "\t\t\telse:\n",
    "\t\t\t\tfunctionV[i] = default\n",
    "\t\tif(len(neuronNumV) > 0):\n",
    "\t\t\tself.layers.append(Layer(featureNum, neuronNumV[0],functionV[0]))\n",
    "\t\t\tfor i in range(1, len(neuronNumV)):\n",
    "\t\t \t\tself.layers.append(Layer(neuronNumV[i-1], neuronNumV[i],functionV[i]))\n",
    "\t\t\tself.layers.append(Layer(neuronNumV[-1], classNum, default))\t# Last layer. Input of the cost function.\n",
    "\t\telse:\n",
    "\t\t\tself.layers.append(Layer(featureNum, classNum, default))\t# Single layer network\n",
    "\t# feauters is the feature is a feature vector. It must have the same size of the featureNum above\n",
    "\t# y is the class number\n",
    "\t# feedforwards data\t\t\n",
    "\t# returns last layers outputs\n",
    "\tdef feedForward(self, features, y):\n",
    "\t\tself.layers[0].inpV = features\n",
    "\t\tfor i in range(1,len(self.layers)):\n",
    "\t\t\tself.layers[i].inpV = self.layers[i-1].forward()\n",
    "\t\treturn self.layers[-1].forward()\n",
    "\t\t\n",
    "\t# Backpropagates\n",
    "\t# Returns deltas. This deltas will be used for weight updating\n",
    "\tdef backProp(self, vector, y):\n",
    "\t\tglobal error\n",
    "\t\twder= []\n",
    "\t\tbder = []\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\twder.append(np.zeros(layer.weights.shape)) ## each neuron has its own lamda value\n",
    "\t\t\tbder.append(np.zeros(len(layer.bias.shape)))\n",
    "\t\t#print(vector)\n",
    "\t\tvector = softmax(vector)\n",
    "\t\t#print(vector)\n",
    "\t\t#quit()\n",
    "\t\terror = crossEntropyError(vector, y) + error\n",
    "\t\t#print( crossEntropyError(vector, y))\n",
    "\t\tdelta = softmaxDelta(vector, y)\n",
    "\t\twder[-1] = np.dot(self.layers[-1].weights.transpose(), delta)\n",
    "\t\tbder[-1] = delta\t\n",
    "\t\tlength = len(self.layers) - 1\n",
    "\t\tfor i in range(length):\n",
    "\t\t\tcur = length - i - 1\n",
    "\t\t\tlayer = self.layers[cur]\n",
    "\t\t\tinpBefore = self.layers[cur + 1].inpV\n",
    "\t\t\tweightBefore = self.layers[cur + 1].weights\n",
    "\t\t\tdelta = layer.findDelta(inpBefore, weightBefore, delta)\n",
    "\t\t\twder[cur] =  np.dot(self.layers[cur].weights.transpose(), delta)\n",
    "\t\t\tbder[cur] = delta\n",
    "\t\treturn (wder, bder)\n",
    "\t\t\n",
    "\t# miniBatch is a tuple list corrosponds of input feautures and correct classes\n",
    "\t# return error for that batch\n",
    "\tdef miniBatchUpdate(self, minibatch): \n",
    "\t\tglobal rate\n",
    "\t\twder = []\n",
    "\t\tbder = []\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\twder.append(np.zeros(layer.weights.shape)) ## each neuron has its own lamda value\n",
    "\t\t\tbder.append(np.zeros(layer.bias.shape))\n",
    "\t\tfor inp in minibatch:\n",
    "\t\t\tfeature = inp[0]\n",
    "\t\t\ty = inp[1]\n",
    "\t\t\tlastAct= self.feedForward(feature, y)\n",
    "\t\t#\tprint(lastAct); print(\"*****\")\n",
    "\t\t\t(wderPass,bderPass) = self.backProp(lastAct, y)\n",
    "\t\t\twder = [dw + dwp for dw,dwp in zip(wder, wderPass)]\n",
    "\t\t\tbder = [bw + bwp for bw, bwp in zip(bder, bderPass)]\t\t\t\n",
    "\t\twder = [layer/len(wder) for layer in wder]\n",
    "\t\tbder = [layer/len(bder) for layer in bder] \n",
    "\t\tfor i in range(len(self.layers)):\n",
    "\t\t\tself.layers[i].weights = self.layers[i].weights - rate*wder[i]\n",
    "\t\t\tself.layers[i].bias = self.layers[i].bias - rate*bder[i]\n",
    "\t\t\t\n",
    "\tdef train(self, inp, miniBatchSize, epoch):\n",
    "\t\tglobal error\n",
    "\t\tfor e in range(epoch):\n",
    "\t\t\terror = 0\n",
    "\t\t\trd.shuffle(inp)\n",
    "\t\t\tminiBatches = [inp[k:k+miniBatchSize] for k in range(0,len(inp), miniBatchSize)]\n",
    "\t\t\tfor batch in miniBatches:\n",
    "\t\t\t\tself.miniBatchUpdate(batch)\n",
    "\t\t\terror = error / len(inp)\n",
    "\t\t\tprint(\"epoch is \" + str(e) + \"\\terror is \" + str(error))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems\n",
    "\n",
    "&emsp;I have implemented the algorithm with mini batch gradient descent. Sometimes derivatives may become infinity or nan. To solve this problem<br>\n",
    "I multiply the initialization weights with a constant. This makes derivatives to become reasonable numbers.\n",
    "&emsp;Sometimes, epoch errors starts to stabilize soon. In that case, I decrement the learning rate and try again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
